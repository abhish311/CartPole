{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONLY KERAS VERSION < 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making New Environment For CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43L]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "env.seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time For Making a Single Layer Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "flatten_1 (Flatten)              (None, 4)             0           flatten_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 16)            80          flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 16)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             34          activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 2)             0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murky/anaconda2/lib/python2.7/site-packages/keras_rl-0.3.1-py2.7.egg/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13/5000: episode: 1, duration: 8.073s, episode steps: 13, steps per second: 2, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.109 [-2.811, 1.747], loss: 0.672139, mean_absolute_error: 0.702761, mean_q: 0.232073\n",
      "   33/5000: episode: 2, duration: 0.205s, episode steps: 20, steps per second: 98, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.123 [-0.344, 0.964], loss: 0.496271, mean_absolute_error: 0.578393, mean_q: 0.298828\n",
      "   57/5000: episode: 3, duration: 0.271s, episode steps: 24, steps per second: 89, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.008 [-3.157, 2.322], loss: 0.413725, mean_absolute_error: 0.512727, mean_q: 0.346002\n",
      "   65/5000: episode: 4, duration: 0.087s, episode steps: 8, steps per second: 92, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.549, 1.557], loss: 0.347167, mean_absolute_error: 0.481412, mean_q: 0.566452\n",
      "   75/5000: episode: 5, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.103, 1.967], loss: 0.326386, mean_absolute_error: 0.476965, mean_q: 0.626794\n",
      "   84/5000: episode: 6, duration: 0.102s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.160 [-2.332, 1.412], loss: 0.306590, mean_absolute_error: 0.467469, mean_q: 0.826311\n",
      "   95/5000: episode: 7, duration: 0.104s, episode steps: 11, steps per second: 106, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.116 [-2.791, 1.768], loss: 0.313762, mean_absolute_error: 0.503870, mean_q: 0.878423\n",
      "  106/5000: episode: 8, duration: 0.114s, episode steps: 11, steps per second: 97, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.143 [-2.757, 1.730], loss: 0.311835, mean_absolute_error: 0.538377, mean_q: 1.003126\n",
      "  115/5000: episode: 9, duration: 0.096s, episode steps: 9, steps per second: 94, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.766, 1.754], loss: 0.296873, mean_absolute_error: 0.580415, mean_q: 1.047095\n",
      "  125/5000: episode: 10, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.010, 1.953], loss: 0.287736, mean_absolute_error: 0.600286, mean_q: 1.203055\n",
      "  134/5000: episode: 11, duration: 0.114s, episode steps: 9, steps per second: 79, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-2.877, 1.795], loss: 0.330792, mean_absolute_error: 0.668509, mean_q: 1.290184\n",
      "  143/5000: episode: 12, duration: 0.101s, episode steps: 9, steps per second: 89, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.130 [-2.089, 1.351], loss: 0.272377, mean_absolute_error: 0.676471, mean_q: 1.296221\n",
      "  152/5000: episode: 13, duration: 0.089s, episode steps: 9, steps per second: 101, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.833, 1.773], loss: 0.280416, mean_absolute_error: 0.707449, mean_q: 1.321076\n",
      "  162/5000: episode: 14, duration: 0.094s, episode steps: 10, steps per second: 106, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.063, 1.947], loss: 0.353806, mean_absolute_error: 0.777810, mean_q: 1.501260\n",
      "  173/5000: episode: 15, duration: 0.147s, episode steps: 11, steps per second: 75, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.123 [-2.386, 1.361], loss: 0.320012, mean_absolute_error: 0.804459, mean_q: 1.564927\n",
      "  183/5000: episode: 16, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-3.008, 1.953], loss: 0.317421, mean_absolute_error: 0.862767, mean_q: 1.656886\n",
      "  193/5000: episode: 17, duration: 0.104s, episode steps: 10, steps per second: 97, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.126 [-2.551, 1.587], loss: 0.339241, mean_absolute_error: 0.898383, mean_q: 1.758578\n",
      "  202/5000: episode: 18, duration: 0.094s, episode steps: 9, steps per second: 96, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.784, 1.715], loss: 0.402152, mean_absolute_error: 0.974337, mean_q: 1.848749\n",
      "  212/5000: episode: 19, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-3.034, 1.931], loss: 0.322514, mean_absolute_error: 0.961951, mean_q: 1.779576\n",
      "  220/5000: episode: 20, duration: 0.094s, episode steps: 8, steps per second: 85, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.534, 1.536], loss: 0.409267, mean_absolute_error: 1.026327, mean_q: 1.888253\n",
      "  229/5000: episode: 21, duration: 0.096s, episode steps: 9, steps per second: 94, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.819, 1.751], loss: 0.401055, mean_absolute_error: 1.063625, mean_q: 1.967179\n",
      "  238/5000: episode: 22, duration: 0.137s, episode steps: 9, steps per second: 66, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.785, 1.767], loss: 0.395716, mean_absolute_error: 1.079910, mean_q: 1.978323\n",
      "  250/5000: episode: 23, duration: 0.137s, episode steps: 12, steps per second: 88, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.126 [-3.024, 1.951], loss: 0.412243, mean_absolute_error: 1.120668, mean_q: 2.062504\n",
      "  260/5000: episode: 24, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.044, 1.957], loss: 0.383244, mean_absolute_error: 1.163258, mean_q: 2.172037\n",
      "  270/5000: episode: 25, duration: 0.097s, episode steps: 10, steps per second: 104, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-3.018, 1.993], loss: 0.420117, mean_absolute_error: 1.195070, mean_q: 2.225339\n",
      "  278/5000: episode: 26, duration: 0.079s, episode steps: 8, steps per second: 101, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.593, 1.571], loss: 0.351806, mean_absolute_error: 1.219389, mean_q: 2.294127\n",
      "  288/5000: episode: 27, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-3.036, 1.916], loss: 0.433148, mean_absolute_error: 1.262087, mean_q: 2.416494\n",
      "  298/5000: episode: 28, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.128 [-2.523, 1.535], loss: 0.372547, mean_absolute_error: 1.275754, mean_q: 2.475624\n",
      "  307/5000: episode: 29, duration: 0.096s, episode steps: 9, steps per second: 94, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.165 [-2.799, 1.712], loss: 0.464928, mean_absolute_error: 1.364009, mean_q: 2.602260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  317/5000: episode: 30, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.166 [-3.119, 1.953], loss: 0.365766, mean_absolute_error: 1.352933, mean_q: 2.568481\n",
      "  327/5000: episode: 31, duration: 0.106s, episode steps: 10, steps per second: 95, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.087, 1.952], loss: 0.367804, mean_absolute_error: 1.410998, mean_q: 2.687538\n",
      "  337/5000: episode: 32, duration: 0.094s, episode steps: 10, steps per second: 106, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.030, 1.996], loss: 0.425276, mean_absolute_error: 1.456456, mean_q: 2.805487\n",
      "  345/5000: episode: 33, duration: 0.111s, episode steps: 8, steps per second: 72, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.580, 1.528], loss: 0.500503, mean_absolute_error: 1.518922, mean_q: 2.835174\n",
      "  353/5000: episode: 34, duration: 0.082s, episode steps: 8, steps per second: 97, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.582, 1.581], loss: 0.537610, mean_absolute_error: 1.588022, mean_q: 2.864293\n",
      "  364/5000: episode: 35, duration: 0.146s, episode steps: 11, steps per second: 76, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.124 [-2.879, 1.777], loss: 0.405973, mean_absolute_error: 1.539291, mean_q: 2.838517\n",
      "  377/5000: episode: 36, duration: 0.126s, episode steps: 13, steps per second: 103, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.116 [-2.844, 1.778], loss: 0.362620, mean_absolute_error: 1.543882, mean_q: 2.974028\n",
      "  386/5000: episode: 37, duration: 0.108s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-2.766, 1.800], loss: 0.448647, mean_absolute_error: 1.634846, mean_q: 3.103536\n",
      "  396/5000: episode: 38, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.112 [-2.968, 1.944], loss: 0.468058, mean_absolute_error: 1.685947, mean_q: 3.099700\n",
      "  406/5000: episode: 39, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.104 [-2.997, 1.998], loss: 0.457863, mean_absolute_error: 1.723809, mean_q: 3.095837\n",
      "  418/5000: episode: 40, duration: 0.138s, episode steps: 12, steps per second: 87, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.111 [-2.571, 1.548], loss: 0.357890, mean_absolute_error: 1.691883, mean_q: 3.171474\n",
      "  427/5000: episode: 41, duration: 0.087s, episode steps: 9, steps per second: 103, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.775, 1.774], loss: 0.358084, mean_absolute_error: 1.741644, mean_q: 3.294047\n",
      "  435/5000: episode: 42, duration: 0.082s, episode steps: 8, steps per second: 98, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-2.542, 1.518], loss: 0.469219, mean_absolute_error: 1.793959, mean_q: 3.338895\n",
      "  443/5000: episode: 43, duration: 0.091s, episode steps: 8, steps per second: 88, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.172 [-2.565, 1.526], loss: 0.353540, mean_absolute_error: 1.757317, mean_q: 3.272832\n",
      "  455/5000: episode: 44, duration: 0.118s, episode steps: 12, steps per second: 102, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.092 [-2.339, 1.574], loss: 0.319556, mean_absolute_error: 1.781284, mean_q: 3.328004\n",
      "  465/5000: episode: 45, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.499, 1.572], loss: 0.304761, mean_absolute_error: 1.797927, mean_q: 3.479462\n",
      "  475/5000: episode: 46, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.129 [-2.492, 1.574], loss: 0.304672, mean_absolute_error: 1.839997, mean_q: 3.613391\n",
      "  485/5000: episode: 47, duration: 0.095s, episode steps: 10, steps per second: 106, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.143 [-2.438, 1.525], loss: 0.346054, mean_absolute_error: 1.876234, mean_q: 3.625928\n",
      "  494/5000: episode: 48, duration: 0.096s, episode steps: 9, steps per second: 94, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.115 [-1.905, 1.209], loss: 0.364108, mean_absolute_error: 1.928364, mean_q: 3.601656\n",
      "  505/5000: episode: 49, duration: 0.113s, episode steps: 11, steps per second: 97, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.118 [-1.864, 1.169], loss: 0.330445, mean_absolute_error: 1.937430, mean_q: 3.608217\n",
      "  515/5000: episode: 50, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.131 [-1.900, 1.148], loss: 0.361009, mean_absolute_error: 1.977849, mean_q: 3.692378\n",
      "  524/5000: episode: 51, duration: 0.093s, episode steps: 9, steps per second: 97, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.130 [-1.933, 1.218], loss: 0.299255, mean_absolute_error: 1.973759, mean_q: 3.798124\n",
      "  535/5000: episode: 52, duration: 0.130s, episode steps: 11, steps per second: 85, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.126 [-2.179, 1.332], loss: 0.331313, mean_absolute_error: 1.968776, mean_q: 3.882696\n",
      "  545/5000: episode: 53, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.126 [-2.117, 1.355], loss: 0.297536, mean_absolute_error: 2.009367, mean_q: 3.931916\n",
      "  555/5000: episode: 54, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.126 [-2.183, 1.374], loss: 0.327034, mean_absolute_error: 2.023563, mean_q: 4.021752\n",
      "  566/5000: episode: 55, duration: 0.109s, episode steps: 11, steps per second: 101, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.121 [-2.133, 1.321], loss: 0.369014, mean_absolute_error: 2.076492, mean_q: 4.081610\n",
      "  579/5000: episode: 56, duration: 0.147s, episode steps: 13, steps per second: 88, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.101 [-2.180, 1.388], loss: 0.316598, mean_absolute_error: 2.056336, mean_q: 4.054212\n",
      "  588/5000: episode: 57, duration: 0.089s, episode steps: 9, steps per second: 101, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.123 [-2.266, 1.401], loss: 0.295685, mean_absolute_error: 2.076444, mean_q: 4.153632\n",
      "  598/5000: episode: 58, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.143 [-2.201, 1.334], loss: 0.396746, mean_absolute_error: 2.133979, mean_q: 4.197871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  607/5000: episode: 59, duration: 0.098s, episode steps: 9, steps per second: 92, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.137 [-1.992, 1.177], loss: 0.372353, mean_absolute_error: 2.160094, mean_q: 4.101202\n",
      "  618/5000: episode: 60, duration: 0.111s, episode steps: 11, steps per second: 99, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.127 [-1.928, 1.190], loss: 0.338922, mean_absolute_error: 2.177845, mean_q: 4.200922\n",
      "  627/5000: episode: 61, duration: 0.085s, episode steps: 9, steps per second: 105, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.145 [-2.208, 1.336], loss: 0.356563, mean_absolute_error: 2.197706, mean_q: 4.353114\n",
      "  638/5000: episode: 62, duration: 0.123s, episode steps: 11, steps per second: 90, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.121 [-2.405, 1.544], loss: 0.261783, mean_absolute_error: 2.149321, mean_q: 4.504005\n",
      "  647/5000: episode: 63, duration: 0.094s, episode steps: 9, steps per second: 96, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.152 [-2.267, 1.323], loss: 0.355617, mean_absolute_error: 2.176495, mean_q: 4.457385\n",
      "  661/5000: episode: 64, duration: 0.150s, episode steps: 14, steps per second: 94, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.086 [-2.163, 1.407], loss: 0.266690, mean_absolute_error: 2.184402, mean_q: 4.488642\n",
      "  674/5000: episode: 65, duration: 0.144s, episode steps: 13, steps per second: 91, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.069 [-2.258, 1.583], loss: 0.360194, mean_absolute_error: 2.274033, mean_q: 4.548308\n",
      "  684/5000: episode: 66, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.125 [-2.167, 1.348], loss: 0.332875, mean_absolute_error: 2.294667, mean_q: 4.538831\n",
      "  697/5000: episode: 67, duration: 0.133s, episode steps: 13, steps per second: 98, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.093 [-2.071, 1.325], loss: 0.324529, mean_absolute_error: 2.337976, mean_q: 4.657873\n",
      "  706/5000: episode: 68, duration: 0.093s, episode steps: 9, steps per second: 97, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.146 [-2.178, 1.364], loss: 0.322406, mean_absolute_error: 2.385037, mean_q: 4.735513\n",
      "  715/5000: episode: 69, duration: 0.096s, episode steps: 9, steps per second: 93, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.149 [-2.195, 1.374], loss: 0.320033, mean_absolute_error: 2.369736, mean_q: 4.668203\n",
      "  724/5000: episode: 70, duration: 0.087s, episode steps: 9, steps per second: 104, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.112 [-2.147, 1.418], loss: 0.285440, mean_absolute_error: 2.441903, mean_q: 4.937536\n",
      "  734/5000: episode: 71, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.138 [-2.561, 1.552], loss: 0.352747, mean_absolute_error: 2.414306, mean_q: 4.766691\n",
      "  745/5000: episode: 72, duration: 0.121s, episode steps: 11, steps per second: 91, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.100 [-2.125, 1.373], loss: 0.361529, mean_absolute_error: 2.404593, mean_q: 4.735292\n",
      "  753/5000: episode: 73, duration: 0.104s, episode steps: 8, steps per second: 77, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.152 [-2.179, 1.329], loss: 0.255295, mean_absolute_error: 2.471262, mean_q: 4.954015\n",
      "  763/5000: episode: 74, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-2.100, 1.348], loss: 0.292672, mean_absolute_error: 2.467183, mean_q: 4.898206\n",
      "  772/5000: episode: 75, duration: 0.105s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.308, 1.412], loss: 0.353570, mean_absolute_error: 2.555057, mean_q: 4.969763\n",
      "  783/5000: episode: 76, duration: 0.127s, episode steps: 11, steps per second: 87, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.118 [-1.876, 1.141], loss: 0.341434, mean_absolute_error: 2.571149, mean_q: 4.992954\n",
      "  794/5000: episode: 77, duration: 0.142s, episode steps: 11, steps per second: 77, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.135 [-2.244, 1.369], loss: 0.253255, mean_absolute_error: 2.574300, mean_q: 5.076775\n",
      "  804/5000: episode: 78, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.107 [-1.931, 1.193], loss: 0.483228, mean_absolute_error: 2.621817, mean_q: 5.073107\n",
      "  813/5000: episode: 79, duration: 0.110s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.132 [-1.986, 1.223], loss: 0.323954, mean_absolute_error: 2.658262, mean_q: 5.209938\n",
      "  822/5000: episode: 80, duration: 0.108s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.142 [-1.907, 1.150], loss: 0.393931, mean_absolute_error: 2.651884, mean_q: 5.130685\n",
      "  832/5000: episode: 81, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.115 [-2.012, 1.187], loss: 0.235735, mean_absolute_error: 2.677408, mean_q: 5.329784\n",
      "  841/5000: episode: 82, duration: 0.117s, episode steps: 9, steps per second: 77, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.144 [-1.944, 1.143], loss: 0.472626, mean_absolute_error: 2.763816, mean_q: 5.344133\n",
      "  852/5000: episode: 83, duration: 0.143s, episode steps: 11, steps per second: 77, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.115 [-1.822, 1.150], loss: 0.376396, mean_absolute_error: 2.768554, mean_q: 5.306205\n",
      "  863/5000: episode: 84, duration: 0.140s, episode steps: 11, steps per second: 78, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.106 [-1.827, 1.173], loss: 0.276733, mean_absolute_error: 2.718484, mean_q: 5.209301\n",
      "  873/5000: episode: 85, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.117 [-2.090, 1.373], loss: 0.502649, mean_absolute_error: 2.829132, mean_q: 5.429074\n",
      "  884/5000: episode: 86, duration: 0.135s, episode steps: 11, steps per second: 81, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.115 [-1.759, 0.993], loss: 0.429230, mean_absolute_error: 2.793988, mean_q: 5.278708\n",
      "  897/5000: episode: 87, duration: 0.177s, episode steps: 13, steps per second: 73, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.111 [-2.102, 1.330], loss: 0.305507, mean_absolute_error: 2.803088, mean_q: 5.383122\n",
      "  908/5000: episode: 88, duration: 0.136s, episode steps: 11, steps per second: 81, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.120 [-1.884, 1.188], loss: 0.257883, mean_absolute_error: 2.804419, mean_q: 5.446047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  919/5000: episode: 89, duration: 0.133s, episode steps: 11, steps per second: 83, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.111 [-1.832, 1.158], loss: 0.260476, mean_absolute_error: 2.855440, mean_q: 5.597764\n",
      "  927/5000: episode: 90, duration: 0.106s, episode steps: 8, steps per second: 76, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.157 [-2.010, 1.184], loss: 0.225982, mean_absolute_error: 2.816130, mean_q: 5.539452\n",
      "  936/5000: episode: 91, duration: 0.096s, episode steps: 9, steps per second: 93, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.114 [-1.877, 1.169], loss: 0.245550, mean_absolute_error: 2.816853, mean_q: 5.519217\n",
      "  945/5000: episode: 92, duration: 0.141s, episode steps: 9, steps per second: 64, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.134 [-1.972, 1.224], loss: 0.244005, mean_absolute_error: 2.827199, mean_q: 5.524285\n",
      "  955/5000: episode: 93, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.137 [-2.692, 1.727], loss: 0.486071, mean_absolute_error: 2.864700, mean_q: 5.489152\n",
      "  967/5000: episode: 94, duration: 0.147s, episode steps: 12, steps per second: 81, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.081 [-1.705, 1.196], loss: 0.339625, mean_absolute_error: 2.939005, mean_q: 5.685518\n",
      "  979/5000: episode: 95, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.095 [-1.663, 1.033], loss: 0.338280, mean_absolute_error: 2.973633, mean_q: 5.691781\n",
      "  991/5000: episode: 96, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.100 [-1.613, 0.995], loss: 0.315609, mean_absolute_error: 3.026200, mean_q: 5.809563\n",
      " 1005/5000: episode: 97, duration: 0.168s, episode steps: 14, steps per second: 83, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.087 [-1.581, 0.986], loss: 0.258541, mean_absolute_error: 3.024435, mean_q: 5.784255\n",
      " 1015/5000: episode: 98, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.130 [-1.862, 1.197], loss: 0.223230, mean_absolute_error: 3.022643, mean_q: 5.802032\n",
      " 1024/5000: episode: 99, duration: 0.117s, episode steps: 9, steps per second: 77, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.125 [-1.736, 1.005], loss: 0.250051, mean_absolute_error: 3.043709, mean_q: 5.822856\n",
      " 1036/5000: episode: 100, duration: 0.143s, episode steps: 12, steps per second: 84, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.116 [-1.677, 1.006], loss: 0.263857, mean_absolute_error: 3.052319, mean_q: 5.833362\n",
      " 1048/5000: episode: 101, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.114 [-1.648, 0.968], loss: 0.480754, mean_absolute_error: 3.109315, mean_q: 5.903326\n",
      " 1061/5000: episode: 102, duration: 0.166s, episode steps: 13, steps per second: 78, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.094 [-1.700, 1.008], loss: 0.246422, mean_absolute_error: 3.102091, mean_q: 5.888189\n",
      " 1073/5000: episode: 103, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.108 [-1.497, 0.827], loss: 0.334991, mean_absolute_error: 3.110563, mean_q: 5.910482\n",
      " 1084/5000: episode: 104, duration: 0.137s, episode steps: 11, steps per second: 80, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.111 [-1.760, 0.982], loss: 0.305434, mean_absolute_error: 3.070164, mean_q: 5.764073\n",
      " 1098/5000: episode: 105, duration: 0.179s, episode steps: 14, steps per second: 78, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.087 [-1.471, 0.785], loss: 0.389449, mean_absolute_error: 3.131070, mean_q: 5.843816\n",
      " 1112/5000: episode: 106, duration: 0.146s, episode steps: 14, steps per second: 96, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.107 [-1.243, 0.804], loss: 0.386701, mean_absolute_error: 3.271284, mean_q: 6.133385\n",
      " 1122/5000: episode: 107, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.909, 3.092], loss: 0.259790, mean_absolute_error: 3.178686, mean_q: 5.968303\n",
      " 1134/5000: episode: 108, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.100 [-1.742, 1.016], loss: 0.546195, mean_absolute_error: 3.296001, mean_q: 6.204290\n",
      " 1144/5000: episode: 109, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.524, 2.548], loss: 0.282060, mean_absolute_error: 3.333446, mean_q: 6.304128\n",
      " 1158/5000: episode: 110, duration: 0.158s, episode steps: 14, steps per second: 88, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.114 [-1.562, 0.754], loss: 0.385404, mean_absolute_error: 3.298093, mean_q: 6.205712\n",
      " 1167/5000: episode: 111, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.785, 2.872], loss: 0.250286, mean_absolute_error: 3.381002, mean_q: 6.440544\n",
      " 1176/5000: episode: 112, duration: 0.093s, episode steps: 9, steps per second: 97, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.120 [-1.398, 2.271], loss: 0.272781, mean_absolute_error: 3.340821, mean_q: 6.314810\n",
      " 1193/5000: episode: 113, duration: 0.213s, episode steps: 17, steps per second: 80, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.065 [-1.399, 0.789], loss: 0.779595, mean_absolute_error: 3.446283, mean_q: 6.459379\n",
      " 1203/5000: episode: 114, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.119 [-1.475, 0.767], loss: 0.632493, mean_absolute_error: 3.376762, mean_q: 6.352468\n",
      " 1219/5000: episode: 115, duration: 0.204s, episode steps: 16, steps per second: 78, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.068 [-1.403, 0.776], loss: 0.623825, mean_absolute_error: 3.420844, mean_q: 6.438697\n",
      " 1232/5000: episode: 116, duration: 0.163s, episode steps: 13, steps per second: 80, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.113 [-1.427, 0.793], loss: 0.261218, mean_absolute_error: 3.423947, mean_q: 6.578353\n",
      " 1245/5000: episode: 117, duration: 0.155s, episode steps: 13, steps per second: 84, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.089 [-1.344, 0.786], loss: 0.385571, mean_absolute_error: 3.420130, mean_q: 6.500274\n",
      " 1261/5000: episode: 118, duration: 0.164s, episode steps: 16, steps per second: 98, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.078 [-1.390, 0.779], loss: 0.400670, mean_absolute_error: 3.536646, mean_q: 6.746981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1277/5000: episode: 119, duration: 0.204s, episode steps: 16, steps per second: 79, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.084 [-1.424, 0.755], loss: 0.322163, mean_absolute_error: 3.535089, mean_q: 6.772120\n",
      " 1290/5000: episode: 120, duration: 0.138s, episode steps: 13, steps per second: 94, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.134 [-1.297, 0.565], loss: 0.450723, mean_absolute_error: 3.658382, mean_q: 7.015777\n",
      " 1303/5000: episode: 121, duration: 0.172s, episode steps: 13, steps per second: 76, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.098 [-1.305, 0.830], loss: 0.383727, mean_absolute_error: 3.674816, mean_q: 7.075556\n",
      " 1321/5000: episode: 122, duration: 0.224s, episode steps: 18, steps per second: 80, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.073 [-1.210, 0.584], loss: 0.573683, mean_absolute_error: 3.638152, mean_q: 6.917859\n",
      " 1335/5000: episode: 123, duration: 0.150s, episode steps: 14, steps per second: 94, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.119 [-1.216, 0.747], loss: 0.371946, mean_absolute_error: 3.709613, mean_q: 7.013503\n",
      " 1347/5000: episode: 124, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.143 [-1.258, 0.574], loss: 0.665564, mean_absolute_error: 3.702331, mean_q: 6.993505\n",
      " 1361/5000: episode: 125, duration: 0.174s, episode steps: 14, steps per second: 81, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.104 [-1.176, 0.561], loss: 0.438136, mean_absolute_error: 3.731166, mean_q: 7.104014\n",
      " 1371/5000: episode: 126, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.124 [-1.600, 2.594], loss: 0.472594, mean_absolute_error: 3.737111, mean_q: 7.128657\n",
      " 1381/5000: episode: 127, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.620, 2.661], loss: 0.914458, mean_absolute_error: 3.853364, mean_q: 7.329953\n",
      " 1395/5000: episode: 128, duration: 0.182s, episode steps: 14, steps per second: 77, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.106 [-1.154, 0.558], loss: 0.877198, mean_absolute_error: 3.919876, mean_q: 7.368268\n",
      " 1410/5000: episode: 129, duration: 0.178s, episode steps: 15, steps per second: 84, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.104 [-1.329, 0.597], loss: 0.481998, mean_absolute_error: 3.801576, mean_q: 7.214679\n",
      " 1430/5000: episode: 130, duration: 0.249s, episode steps: 20, steps per second: 80, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.059 [-1.130, 0.603], loss: 0.773886, mean_absolute_error: 3.929487, mean_q: 7.435618\n",
      " 1449/5000: episode: 131, duration: 0.233s, episode steps: 19, steps per second: 82, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.077 [-1.225, 0.620], loss: 0.595182, mean_absolute_error: 3.878646, mean_q: 7.368193\n",
      " 1461/5000: episode: 132, duration: 0.145s, episode steps: 12, steps per second: 83, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.126 [-1.155, 0.607], loss: 0.377320, mean_absolute_error: 3.918062, mean_q: 7.460822\n",
      " 1475/5000: episode: 133, duration: 0.192s, episode steps: 14, steps per second: 73, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.113 [-1.211, 0.551], loss: 0.788280, mean_absolute_error: 4.009612, mean_q: 7.574562\n",
      " 1489/5000: episode: 134, duration: 0.159s, episode steps: 14, steps per second: 88, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.106 [-1.231, 0.621], loss: 1.221957, mean_absolute_error: 4.055066, mean_q: 7.596615\n",
      " 1502/5000: episode: 135, duration: 0.143s, episode steps: 13, steps per second: 91, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.128 [-1.317, 0.587], loss: 0.626427, mean_absolute_error: 3.962362, mean_q: 7.511694\n",
      " 1519/5000: episode: 136, duration: 0.198s, episode steps: 17, steps per second: 86, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.063 [-1.210, 0.828], loss: 0.861328, mean_absolute_error: 3.974869, mean_q: 7.461332\n",
      " 1532/5000: episode: 137, duration: 0.147s, episode steps: 13, steps per second: 89, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.108 [-1.364, 0.774], loss: 0.791298, mean_absolute_error: 4.061313, mean_q: 7.734108\n",
      " 1546/5000: episode: 138, duration: 0.156s, episode steps: 14, steps per second: 90, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.104 [-1.449, 0.745], loss: 0.696251, mean_absolute_error: 4.002893, mean_q: 7.629345\n",
      " 1559/5000: episode: 139, duration: 0.160s, episode steps: 13, steps per second: 81, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.092 [-1.281, 0.799], loss: 0.634393, mean_absolute_error: 3.938450, mean_q: 7.455381\n",
      " 1575/5000: episode: 140, duration: 0.218s, episode steps: 16, steps per second: 73, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.075 [-1.171, 0.641], loss: 0.538242, mean_absolute_error: 4.065975, mean_q: 7.761172\n",
      " 1593/5000: episode: 141, duration: 0.195s, episode steps: 18, steps per second: 92, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.085 [-1.147, 0.734], loss: 0.893660, mean_absolute_error: 4.109381, mean_q: 7.707759\n",
      " 1610/5000: episode: 142, duration: 0.203s, episode steps: 17, steps per second: 84, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.109 [-1.285, 0.561], loss: 0.730266, mean_absolute_error: 4.110559, mean_q: 7.817271\n",
      " 1627/5000: episode: 143, duration: 0.185s, episode steps: 17, steps per second: 92, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.089 [-1.056, 0.408], loss: 0.622449, mean_absolute_error: 4.096194, mean_q: 7.737542\n",
      " 1650/5000: episode: 144, duration: 0.264s, episode steps: 23, steps per second: 87, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.066 [-1.193, 0.573], loss: 0.535669, mean_absolute_error: 4.194386, mean_q: 7.938787\n",
      " 1669/5000: episode: 145, duration: 0.192s, episode steps: 19, steps per second: 99, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.069 [-1.222, 0.622], loss: 0.606198, mean_absolute_error: 4.251805, mean_q: 8.111796\n",
      " 1689/5000: episode: 146, duration: 0.251s, episode steps: 20, steps per second: 80, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.093 [-1.102, 0.419], loss: 0.485146, mean_absolute_error: 4.265451, mean_q: 8.167990\n",
      " 1710/5000: episode: 147, duration: 0.249s, episode steps: 21, steps per second: 84, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.079 [-0.951, 0.570], loss: 0.599883, mean_absolute_error: 4.320575, mean_q: 8.295080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1732/5000: episode: 148, duration: 0.272s, episode steps: 22, steps per second: 81, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-0.953, 0.225], loss: 0.700739, mean_absolute_error: 4.338141, mean_q: 8.219877\n",
      " 1758/5000: episode: 149, duration: 0.295s, episode steps: 26, steps per second: 88, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.076 [-0.598, 1.332], loss: 0.858818, mean_absolute_error: 4.419960, mean_q: 8.396388\n",
      " 1781/5000: episode: 150, duration: 0.282s, episode steps: 23, steps per second: 82, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.068 [-0.894, 0.409], loss: 0.631219, mean_absolute_error: 4.408230, mean_q: 8.439411\n",
      " 1813/5000: episode: 151, duration: 0.367s, episode steps: 32, steps per second: 87, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-0.979, 0.213], loss: 0.688799, mean_absolute_error: 4.415771, mean_q: 8.465428\n",
      " 1837/5000: episode: 152, duration: 0.321s, episode steps: 24, steps per second: 75, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-0.944, 0.202], loss: 1.001170, mean_absolute_error: 4.641978, mean_q: 8.822792\n",
      " 1861/5000: episode: 153, duration: 0.316s, episode steps: 24, steps per second: 76, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-0.904, 0.416], loss: 0.516163, mean_absolute_error: 4.531731, mean_q: 8.694901\n",
      " 1905/5000: episode: 154, duration: 0.535s, episode steps: 44, steps per second: 82, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-0.909, 0.366], loss: 0.726513, mean_absolute_error: 4.702474, mean_q: 9.013968\n",
      " 1927/5000: episode: 155, duration: 0.270s, episode steps: 22, steps per second: 81, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.254, 0.906], loss: 0.971325, mean_absolute_error: 4.634152, mean_q: 8.785127\n",
      " 1961/5000: episode: 156, duration: 0.395s, episode steps: 34, steps per second: 86, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.048 [-1.170, 0.415], loss: 0.711230, mean_absolute_error: 4.661112, mean_q: 8.872283\n",
      " 1985/5000: episode: 157, duration: 0.307s, episode steps: 24, steps per second: 78, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.970, 0.200], loss: 1.052308, mean_absolute_error: 4.777645, mean_q: 9.094476\n",
      " 2013/5000: episode: 158, duration: 0.328s, episode steps: 28, steps per second: 85, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-0.928, 0.188], loss: 0.844512, mean_absolute_error: 4.710101, mean_q: 8.967235\n",
      " 2055/5000: episode: 159, duration: 0.488s, episode steps: 42, steps per second: 86, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.042 [-0.378, 1.144], loss: 1.078536, mean_absolute_error: 4.897320, mean_q: 9.366563\n",
      " 2087/5000: episode: 160, duration: 0.381s, episode steps: 32, steps per second: 84, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.065 [-0.580, 1.327], loss: 0.848245, mean_absolute_error: 4.956071, mean_q: 9.536978\n",
      " 2104/5000: episode: 161, duration: 0.168s, episode steps: 17, steps per second: 101, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.087 [-0.622, 1.357], loss: 1.144327, mean_absolute_error: 5.194386, mean_q: 10.044771\n",
      " 2120/5000: episode: 162, duration: 0.153s, episode steps: 16, steps per second: 105, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.083 [-0.645, 1.287], loss: 0.829818, mean_absolute_error: 5.003071, mean_q: 9.596361\n",
      " 2136/5000: episode: 163, duration: 0.162s, episode steps: 16, steps per second: 99, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.097 [-0.613, 1.341], loss: 1.431409, mean_absolute_error: 5.144471, mean_q: 9.870972\n",
      " 2156/5000: episode: 164, duration: 0.204s, episode steps: 20, steps per second: 98, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.098 [-0.555, 1.217], loss: 1.241831, mean_absolute_error: 5.226461, mean_q: 10.020359\n",
      " 2171/5000: episode: 165, duration: 0.152s, episode steps: 15, steps per second: 99, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.103 [-0.391, 1.045], loss: 1.613801, mean_absolute_error: 5.334616, mean_q: 10.090540\n",
      " 2200/5000: episode: 166, duration: 0.284s, episode steps: 29, steps per second: 102, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.106 [-0.995, 0.370], loss: 1.656423, mean_absolute_error: 5.279216, mean_q: 9.961409\n",
      " 2223/5000: episode: 167, duration: 0.225s, episode steps: 23, steps per second: 102, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.104 [-0.942, 0.591], loss: 1.259760, mean_absolute_error: 5.303632, mean_q: 10.105034\n",
      " 2248/5000: episode: 168, duration: 0.248s, episode steps: 25, steps per second: 101, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.057 [-0.637, 1.403], loss: 1.477771, mean_absolute_error: 5.332810, mean_q: 10.162402\n",
      " 2299/5000: episode: 169, duration: 0.499s, episode steps: 51, steps per second: 102, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.004 [-0.583, 1.255], loss: 1.493107, mean_absolute_error: 5.414790, mean_q: 10.297523\n",
      " 2321/5000: episode: 170, duration: 0.215s, episode steps: 22, steps per second: 102, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.068 [-0.570, 1.209], loss: 1.474320, mean_absolute_error: 5.453340, mean_q: 10.434205\n",
      " 2353/5000: episode: 171, duration: 0.319s, episode steps: 32, steps per second: 100, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.249, 0.904], loss: 1.680950, mean_absolute_error: 5.502058, mean_q: 10.425722\n",
      " 2377/5000: episode: 172, duration: 0.254s, episode steps: 24, steps per second: 94, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-0.941, 0.352], loss: 1.725005, mean_absolute_error: 5.592432, mean_q: 10.629418\n",
      " 2409/5000: episode: 173, duration: 0.392s, episode steps: 32, steps per second: 82, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.092 [-1.007, 0.191], loss: 1.186570, mean_absolute_error: 5.581611, mean_q: 10.674698\n",
      " 2450/5000: episode: 174, duration: 0.450s, episode steps: 41, steps per second: 91, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.049 [-0.887, 0.540], loss: 1.765060, mean_absolute_error: 5.710433, mean_q: 10.818932\n",
      " 2501/5000: episode: 175, duration: 0.642s, episode steps: 51, steps per second: 79, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.091 [-0.342, 1.218], loss: 1.082060, mean_absolute_error: 5.565218, mean_q: 10.671533\n",
      " 2554/5000: episode: 176, duration: 0.530s, episode steps: 53, steps per second: 100, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.081 [-0.926, 0.346], loss: 1.926061, mean_absolute_error: 5.859434, mean_q: 11.121290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2602/5000: episode: 177, duration: 0.444s, episode steps: 48, steps per second: 108, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.037 [-0.545, 1.070], loss: 1.257220, mean_absolute_error: 5.834306, mean_q: 11.208236\n",
      " 2626/5000: episode: 178, duration: 0.247s, episode steps: 24, steps per second: 97, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.257, 0.801], loss: 0.623783, mean_absolute_error: 5.840294, mean_q: 11.315808\n",
      " 2670/5000: episode: 179, duration: 0.424s, episode steps: 44, steps per second: 104, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.070 [-0.421, 1.289], loss: 1.351152, mean_absolute_error: 6.003668, mean_q: 11.558526\n",
      " 2712/5000: episode: 180, duration: 0.463s, episode steps: 42, steps per second: 91, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.081 [-0.775, 0.266], loss: 1.492453, mean_absolute_error: 6.131182, mean_q: 11.764921\n",
      " 2741/5000: episode: 181, duration: 0.386s, episode steps: 29, steps per second: 75, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.105 [-0.871, 0.201], loss: 3.195283, mean_absolute_error: 6.397887, mean_q: 12.069928\n",
      " 2776/5000: episode: 182, duration: 0.413s, episode steps: 35, steps per second: 85, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.078 [-0.841, 0.221], loss: 1.742786, mean_absolute_error: 6.136677, mean_q: 11.672303\n",
      " 2804/5000: episode: 183, duration: 0.331s, episode steps: 28, steps per second: 85, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-0.877, 0.351], loss: 1.933183, mean_absolute_error: 6.169518, mean_q: 11.785138\n",
      " 2827/5000: episode: 184, duration: 0.222s, episode steps: 23, steps per second: 104, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.111 [-1.532, 0.600], loss: 1.991869, mean_absolute_error: 6.112664, mean_q: 11.634976\n",
      " 2861/5000: episode: 185, duration: 0.337s, episode steps: 34, steps per second: 101, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.085 [-0.772, 0.422], loss: 1.495599, mean_absolute_error: 6.229368, mean_q: 11.946355\n",
      " 2902/5000: episode: 186, duration: 0.402s, episode steps: 41, steps per second: 102, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.081 [-0.836, 0.298], loss: 2.464819, mean_absolute_error: 6.384246, mean_q: 12.184569\n",
      " 2938/5000: episode: 187, duration: 0.351s, episode steps: 36, steps per second: 102, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.105 [-0.778, 0.221], loss: 2.478194, mean_absolute_error: 6.402176, mean_q: 12.153271\n",
      " 3006/5000: episode: 188, duration: 0.663s, episode steps: 68, steps per second: 103, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.027 [-1.311, 0.626], loss: 1.444365, mean_absolute_error: 6.375304, mean_q: 12.221520\n",
      " 3052/5000: episode: 189, duration: 0.464s, episode steps: 46, steps per second: 99, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.079 [-0.819, 0.231], loss: 1.750203, mean_absolute_error: 6.267960, mean_q: 11.982272\n",
      " 3090/5000: episode: 190, duration: 0.484s, episode steps: 38, steps per second: 78, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.075 [-0.633, 0.405], loss: 1.639967, mean_absolute_error: 6.468082, mean_q: 12.409749\n",
      " 3245/5000: episode: 191, duration: 1.883s, episode steps: 155, steps per second: 82, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.049 [-0.834, 0.402], loss: 1.626314, mean_absolute_error: 6.574583, mean_q: 12.667116\n",
      " 3283/5000: episode: 192, duration: 0.317s, episode steps: 38, steps per second: 120, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.095 [-0.226, 0.721], loss: 2.686332, mean_absolute_error: 6.907315, mean_q: 13.217727\n",
      " 3322/5000: episode: 193, duration: 0.317s, episode steps: 39, steps per second: 123, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.124 [-0.738, 0.315], loss: 2.080729, mean_absolute_error: 6.880327, mean_q: 13.214006\n",
      " 3382/5000: episode: 194, duration: 0.434s, episode steps: 60, steps per second: 138, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.089 [-0.687, 0.260], loss: 1.973696, mean_absolute_error: 6.937636, mean_q: 13.352860\n",
      " 3443/5000: episode: 195, duration: 0.601s, episode steps: 61, steps per second: 102, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.135 [-0.906, 0.213], loss: 2.301042, mean_absolute_error: 7.124654, mean_q: 13.771908\n",
      " 3506/5000: episode: 196, duration: 0.668s, episode steps: 63, steps per second: 94, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.100 [-0.491, 1.146], loss: 1.321449, mean_absolute_error: 7.117793, mean_q: 13.811802\n",
      " 3562/5000: episode: 197, duration: 0.529s, episode steps: 56, steps per second: 106, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.114 [-0.818, 0.373], loss: 1.374535, mean_absolute_error: 7.237862, mean_q: 14.070305\n",
      " 3608/5000: episode: 198, duration: 0.448s, episode steps: 46, steps per second: 103, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.103 [-0.251, 0.861], loss: 1.852576, mean_absolute_error: 7.438050, mean_q: 14.341209\n",
      " 3681/5000: episode: 199, duration: 0.706s, episode steps: 73, steps per second: 103, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.072 [-0.698, 0.371], loss: 1.921427, mean_absolute_error: 7.478188, mean_q: 14.467471\n",
      " 3725/5000: episode: 200, duration: 0.419s, episode steps: 44, steps per second: 105, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.126 [-0.708, 0.244], loss: 1.812728, mean_absolute_error: 7.688582, mean_q: 14.862227\n",
      " 3777/5000: episode: 201, duration: 0.503s, episode steps: 52, steps per second: 103, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.112 [-0.751, 0.174], loss: 2.780046, mean_absolute_error: 7.689989, mean_q: 14.767366\n",
      " 3831/5000: episode: 202, duration: 0.508s, episode steps: 54, steps per second: 106, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.136 [-0.755, 0.173], loss: 1.875529, mean_absolute_error: 7.677769, mean_q: 14.895043\n",
      " 3901/5000: episode: 203, duration: 0.659s, episode steps: 70, steps per second: 106, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.094 [-0.433, 0.678], loss: 1.939461, mean_absolute_error: 7.775030, mean_q: 15.085870\n",
      " 3993/5000: episode: 204, duration: 0.852s, episode steps: 92, steps per second: 108, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.083 [-0.737, 0.457], loss: 2.044081, mean_absolute_error: 7.914217, mean_q: 15.361621\n",
      " 4134/5000: episode: 205, duration: 1.348s, episode steps: 141, steps per second: 105, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.080 [-0.493, 0.756], loss: 2.099944, mean_absolute_error: 8.143803, mean_q: 15.871438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4293/5000: episode: 206, duration: 1.463s, episode steps: 159, steps per second: 109, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.071 [-0.677, 0.524], loss: 2.283167, mean_absolute_error: 8.426129, mean_q: 16.376465\n",
      " 4399/5000: episode: 207, duration: 1.028s, episode steps: 106, steps per second: 103, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.102 [-0.848, 0.537], loss: 2.291833, mean_absolute_error: 8.631959, mean_q: 16.844940\n",
      " 4543/5000: episode: 208, duration: 1.371s, episode steps: 144, steps per second: 105, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.080 [-0.777, 0.842], loss: 2.547509, mean_absolute_error: 8.947972, mean_q: 17.445082\n",
      " 4601/5000: episode: 209, duration: 0.548s, episode steps: 58, steps per second: 106, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.108 [-0.264, 0.980], loss: 1.966756, mean_absolute_error: 9.065752, mean_q: 17.828476\n",
      " 4650/5000: episode: 210, duration: 0.476s, episode steps: 49, steps per second: 103, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.122 [-0.196, 0.694], loss: 1.957211, mean_absolute_error: 9.155457, mean_q: 17.993292\n",
      " 4738/5000: episode: 211, duration: 0.828s, episode steps: 88, steps per second: 106, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.130 [-0.852, 0.544], loss: 2.202118, mean_absolute_error: 9.404949, mean_q: 18.469240\n",
      " 4846/5000: episode: 212, duration: 1.000s, episode steps: 108, steps per second: 108, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.111 [-0.732, 0.383], loss: 3.153284, mean_absolute_error: 9.574353, mean_q: 18.676138\n",
      " 4958/5000: episode: 213, duration: 1.057s, episode steps: 112, steps per second: 106, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.080 [-0.505, 0.716], loss: 2.360120, mean_absolute_error: 9.737203, mean_q: 19.141298\n",
      "done, took 61.427 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcaa4454490>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 65.000, steps: 65\n",
      "Episode 2: reward: 131.000, steps: 131\n",
      "Episode 3: reward: 94.000, steps: 94\n",
      "Episode 4: reward: 135.000, steps: 135\n",
      "Episode 5: reward: 180.000, steps: 180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcaa78172d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
